{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "246c68d4",
   "metadata": {},
   "source": [
    "## 1. This script intends to prepare the subbasin attributes of Raven hydrological modelling platform using the Physitel inputs/outputs. The script runs on Python version 3.8 and relies heaviliy on geopandas library for geospatial proecsses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3903257",
   "metadata": {},
   "source": [
    "### Section 0: import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a682e67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "import shutil,os\n",
    "import geopandas as gpd\n",
    "from geopandas.tools import sjoin\n",
    "from rasterstats import zonal_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159b7590",
   "metadata": {},
   "source": [
    "### Section 1: Read inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29554c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Troncon_path = r'C:\\Users\\mohbiz1\\Desktop\\Dossier_travail\\Hydrotel\\DEH\\INFO_TRONCON.mat' # the projet database\n",
    "data = sio.loadmat(Troncon_path, struct_as_record=False, squeeze_me=True)\n",
    "region_name = data['SLSO_TRONCON']\n",
    "size = region_name.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b639dc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy of the project directory\n",
    "df = []\n",
    "for i in range(size):\n",
    "    rec = region_name[i]\n",
    "    df.append([rec.NOEUD_AVAL.NUMERO,rec.NOEUD_AMONT.NUMERO,rec.NO_TRONCON,rec.TYPE_NO,rec.LONGUEUR,rec.LARGEUR,rec.UHRH_ASSOCIES,rec.C_MANNING,rec.PENTE_MOYENNE,rec.SUPERFICIE_DRAINEE])\n",
    "Troncon_info= pd.DataFrame(df,columns = ['NODE_AVAL','NODE_AMONT','SubId','TYPE_NO','RivLength','BnkfWidth','ASSOCI_UHRH','Ch_n','RivSlope','SA_Up'])\n",
    "\n",
    "pathtoDirectory = r\"C:\\Users\\mohbiz1\\Desktop\\Dossier_travail\\Hydrotel\\DEH\\MG24HA\\SLSO_MG24HA_2020\\physitel\"\n",
    "workspace = os.path.join(pathtoDirectory+ \"\\HRU\")\n",
    "shutil.copytree(pathtoDirectory,workspace)\n",
    "Troncon_info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d991539f",
   "metadata": {},
   "source": [
    "### Section2: Add subbasin id (SubId) to UHRH shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2f6931",
   "metadata": {},
   "outputs": [],
   "source": [
    "uhrh_fpth = os.path.join(workspace,\"uhrh\"+ \".\" + \"shp\") # The uhrh shape file created by Physitel\n",
    "uhrh = gpd.read_file(uhrh_fpth)\n",
    "uhrh['SubId'] = 0\n",
    "\n",
    "Troncon_info.loc[Troncon_info.TYPE_NO == 2, 'Ch_n'] = 0.  \n",
    "Troncon_info.loc[Troncon_info.TYPE_NO == 2, 'BnkfWidth'] = 0.\n",
    "\n",
    "i=0\n",
    "for i in range(size):\n",
    "    a = Troncon_info['ASSOCI_UHRH'][i]\n",
    "    id = Troncon_info['SubId'][i]\n",
    "#    print ('writing subbasin :', i )\n",
    "    if type(a) is int:\n",
    "        aa = [a]\n",
    "        st = len(aa)\n",
    "        stt = st-1\n",
    "        dict = {i: aa[i] for i in range(0, len(aa))}\n",
    "    else:\n",
    "        al = a.tolist()\n",
    "        st = len(al)  # number of UHRH associated with current reach\n",
    "        stt = st - 1\n",
    "        #create a temporary dictionary\n",
    "        dict = {i: al[i] for i in range(0, len(al))}\n",
    "    for j in range(st):\n",
    "        for index, row in uhrh.iterrows():\n",
    "            if uhrh.loc[index,'ident'] in dict.values():\n",
    "                uhrh.loc[index,'SubId'] = id\n",
    "\n",
    "os.chdir(workspace)\n",
    "uhrh.to_file('uhrh_diss.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382ebc84",
   "metadata": {},
   "source": [
    "### Section 3: Merge the UHRHs based on SubId field. The number of feature classes in the output file should be same sa number of river reaches (Troncons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755e814c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "uhrh_diss = gpd.read_file(os.path.join(workspace,\"uhrh_diss\"+ \".\" + \"shp\"))\n",
    "uhrh_dissolve = uhrh_diss.dissolve(by='SubId')\n",
    "uhrh_dissolve.reset_index(inplace=True)    \n",
    "uhrh_dissolve['BasArea'] = uhrh_dissolve.area  # calculating the Area (m2) of each subbasin       \n",
    "    \n",
    "os.chdir(workspace)\n",
    "uhrh_dissolve.to_file('uhrh_dissolve.shp')\n",
    "\n",
    "# step3: finding the downstream subwatershed ID associated with each uhrh\n",
    "\n",
    "Troncon_info['DowSubId']=-1\n",
    "for i in range(size):\n",
    "    naval = Troncon_info['NODE_AVAL'][i]\n",
    "    for j in range(size):\n",
    "        namont= Troncon_info['NODE_AMONT'][j]\n",
    "        id = Troncon_info['SubId'][j]\n",
    "        if type(namont) is int:\n",
    "            nal = [namont]\n",
    "        else:\n",
    "            nal = namont.tolist()\n",
    "        if naval in nal: # if naval (downstream node) for reach i is upstream node for reach j, then reach j is downstream reach i\n",
    "            Troncon_info.loc[i, 'DowSubId'] = id\n",
    "    \n",
    "Troncon_info['Has_Gauge'] = (Troncon_info['DowSubId'] == -1).astype(int)  #create a boolean indicator to set 1 for gauged \n",
    "#subwatershed and 0 for others\n",
    "Troncon_info['BkfDepth'] = 0.13 * (Troncon_info['SA_Up'] ** 0.4) # taken from equation 10 in paper Fossey et. al., 2015\n",
    "Troncon_info['Lake_Cat']= 0\n",
    "Troncon_info.loc[Troncon_info.TYPE_NO == 2, 'Lake_Cat'] = 1    \n",
    "\n",
    "# TO BE DISCUSSED:\n",
    "# In Troncon_info dataframe, the outlet has the DowSubId of -1, which can be the number of gauge.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b95e85",
   "metadata": {},
   "source": [
    "### Section4: Parametrization lake features using the HyLAKES database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555c692f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pth = r\"C:\\Users\\mohbiz1\\Desktop\\Dossier_travail\\Hydrotel\\HydroLAKES_polys_v10_shp\"\n",
    "pth2 = os.path.join(pth,\"HydroLAKES_polys_v10_Canada2\"+ \".\" + \"shp\") # The clipped version of HyLAKES for Canada\n",
    "HyLAKES_Canada = gpd.read_file(pth2)\n",
    "\n",
    "pth3 = os.path.join(workspace,\"lacs\"+ \".\" + \"shp\") # The lake shape file created by Physitel\n",
    "Hydrotel_lakes = gpd.read_file(pth3)\n",
    "join_lakes_attr = sjoin(HyLAKES_Canada, Hydrotel_lakes, how=\"right\") \n",
    "\n",
    "# Dealing with cases where there are two lakes in subwatershed whereas only one lake is identified in the lacs.shp shapefile \n",
    "# by Hydrotel\n",
    "#finding rows with similar ident (uhrh) value\n",
    "repeatd_ident = join_lakes_attr[join_lakes_attr.duplicated(subset = ['ident'], keep= False)]\n",
    "\n",
    "repeatd_ident.reset_index(level=0,inplace = True)\n",
    "\n",
    "diss_repeat = repeatd_ident.dissolve(by = 'index',aggfunc='sum')\n",
    "\n",
    "diss_repeat['Depth_avg'] = diss_repeat['Vol_total']/diss_repeat['Lake_area'] #recalculating lake average depth\n",
    "diss_repeat['Lake_type'] = 1\n",
    "\n",
    "\n",
    "#replacing this to the repeatd_ident dataframe\n",
    "\n",
    "join_lakes_attr = join_lakes_attr.drop(diss_repeat.index)\n",
    "lake_final = (pd.concat([join_lakes_attr,diss_repeat])).sort_index()\n",
    "lake_final = lake_final.drop(['index_left'], axis=1)\n",
    "\n",
    "os.chdir(workspace)\n",
    "lake_final.to_file('lake_final.shp')\n",
    "\n",
    "# Intersecting with uhrh_dissolve to find the SubId of each lake\n",
    "lake_sub = sjoin(lake_final,uhrh_dissolve,how = 'right',op='within')\n",
    "\n",
    "lake_sub['Lake_Area'] = lake_sub['Lake_area'] * 1000000.  # To convert the area in Km2 in HydroLAKES database to m2\n",
    "lake_sub['LakeVol'] = lake_sub['Vol_total'] / 1000.  # To convert the volume in MCM in HydroLAKES database to km3\n",
    "lake_sub['LakeDepth'] = lake_sub['Depth_avg'] \n",
    "\n",
    "os.chdir(workspace)\n",
    "lake_sub.to_file('uhrh_with_lake.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6668241",
   "metadata": {},
   "source": [
    "### Section5: Add the downstream ID to the shapefile of the created subbasin shapefile (uhrh_diss.shp)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98cbd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "pth4 = os.path.join(workspace,\"uhrh_with_lake\"+ \".\" + \"shp\")    \n",
    "subbasin = gpd.read_file(pth4)\n",
    "\n",
    "subbasin['DowSubId'] = 0\n",
    "subbasin['RivLength'] = 0.0\n",
    "subbasin['BkfWidth'] = 0.0\n",
    "subbasin['BkfDepth'] = 0.0\n",
    "subbasin['Has_Gauge'] = 0.0\n",
    "subbasin['RivSlope'] = 0.0\n",
    "subbasin['Ch_n'] = 0.0\n",
    "subbasin['FloodP_n'] = 0.0\n",
    "subbasin['Lake_Cat'] = 0\n",
    "#Lake data from HydroLAKES database\n",
    "subbasin['HyLakeId'] = subbasin['Hylak_id']\n",
    "\n",
    "j=0\n",
    "for index, row in subbasin.iterrows():\n",
    "    if index > subbasin.index[-1]:\n",
    "        break\n",
    "    subbasin.loc[index,'DowSubId'] = Troncon_info['DowSubId'][j]\n",
    "    subbasin.loc[index,'RivLength'] = Troncon_info['RivLength'][j]\n",
    "    subbasin.loc[index,'BkfDepth'] = Troncon_info['BkfDepth'][j]\n",
    "    subbasin.loc[index,'BkfWidth'] = Troncon_info['BnkfWidth'][j]\n",
    "    subbasin.loc[index,'Has_Gauge'] = Troncon_info['Has_Gauge'][j]\n",
    "    subbasin.loc[index,'RivSlope'] = Troncon_info['RivSlope'][j]\n",
    "    subbasin.loc[index,'Ch_n'] = Troncon_info['Ch_n'][j]\n",
    "    subbasin.loc[index,'FloodP_n'] = Troncon_info['Ch_n'][j]    # to be discussed\n",
    "    subbasin.loc[index,'Lake_Cat'] = Troncon_info['Lake_Cat'][j]\n",
    "    j = j+1\n",
    "\n",
    "os.chdir(workspace)\n",
    "subbasin.to_file('subbasin.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60562a67",
   "metadata": {},
   "source": [
    "### Section6: Calculating BasSlope,BasAspect,, and Mean_Elev of subbasin features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244fecf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Slope\n",
    "\n",
    "os.chdir(workspace)\n",
    "cmd_slope = 'gdaldem  slope altitude.tif slope.tif -compute_edges'\n",
    "os.system(cmd_slope)\n",
    "# slope must be between 0 to 60 degree (http://hydrology.uwaterloo.ca/basinmaker/data/resources/attribute_tables_20210429.pdf)\n",
    "\n",
    "# Aspect\n",
    "os.chdir(workspace)\n",
    "cmd_aspect = 'gdaldem  aspect altitude.tif aspect.tif -trigonometric -compute_edges'\n",
    "os.system(cmd_aspect)\n",
    "\n",
    "\n",
    "# loop over the subbasin features and adding the mean elevation, mean aspect and \n",
    "ss = os.path.join(workspace,\"slope\"+ \".\" + \"tif\") # The lake shape file created by Physitel\n",
    "pth5 = os.path.join(workspace,\"subbasin\"+ \".\" + \"shp\") # The lake shape file created by Physitel\n",
    "subbasin = gpd.read_file(pth5)\n",
    "\n",
    "subbasin = subbasin.join(\n",
    "    pd.DataFrame(\n",
    "        zonal_stats(\n",
    "            vectors=subbasin['geometry'], \n",
    "            raster= ss, \n",
    "            stats=['mean']\n",
    "        )\n",
    "    ),\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "subbasin.loc[subbasin['mean'] < 0 , \"mean\"] = 0 \n",
    "\n",
    "subbasin['BasSlope'] = subbasin['mean']\n",
    "subbasin = subbasin.drop(['mean'], axis=1)\n",
    "\n",
    "#aspect\n",
    "aa = os.path.join(workspace,\"aspect\"+ \".\" + \"tif\") # The lake shape file created by Physitel\n",
    "\n",
    "subbasin = subbasin.join(\n",
    "    pd.DataFrame(\n",
    "        zonal_stats(\n",
    "            vectors=subbasin['geometry'], \n",
    "            raster= aa, \n",
    "            stats=['mean']\n",
    "        )\n",
    "    ),\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "subbasin['BasAspect'] = subbasin['mean']\n",
    "subbasin = subbasin.drop(['mean'], axis=1)\n",
    "\n",
    "#elevation\n",
    "\n",
    "ee = os.path.join(workspace,\"altitude\"+ \".\" + \"tif\") # The lake shape file created by Physitel\n",
    "\n",
    "subbasin = subbasin.join(\n",
    "    pd.DataFrame(\n",
    "        zonal_stats(\n",
    "            vectors=subbasin['geometry'], \n",
    "            raster= ee, \n",
    "            stats=['mean']\n",
    "        )\n",
    "    ),\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "subbasin['MeanElev'] = subbasin['mean']\n",
    "subbasin = subbasin.drop(['mean'], axis=1)\n",
    "\n",
    "# cleaning: removing irrelevant attributes\n",
    "subbasin['Lake_Area'] = subbasin['Lake_Are_1']\n",
    "\n",
    "subbasin = subbasin.drop(['Lake_name','Country','Continent','Poly_src','Grand_id','Lake_area','Shore_len','Shore_dev','Vol_total',\n",
    "                          'Vol_res','Vol_src','Depth_avg','Dis_avg','Res_time','Elevation','Slope_100',\n",
    "                          'Wshd_area','Pour_long','Pour_lat','Shape_Leng','Shape_Area','ident_x','ident_y','Hylak_id','Lake_Are_1'], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67eab170",
   "metadata": {},
   "source": [
    "### Section7: Writing final subbasin map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69973d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(workspace)\n",
    "subbasin.to_file('subbasin_final.shp')\n",
    "\n",
    "\n",
    "for fname in os.listdir(workspace):\n",
    "    if fname.startswith(\"lake_final\"):\n",
    "        os.remove(os.path.join(workspace, fname))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
